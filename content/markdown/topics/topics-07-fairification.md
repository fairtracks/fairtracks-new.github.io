---
title: Track Metadata formats
img:
  - images
  - topics
  - under-construction.png
caption: TBA
ingress: TBA
---

While track files are nowadays a *de facto* standard for storing,
distributing and analyzing genome-wide datasets,
the common practices for this data type do not currently comply with the FAIR data principles.
One of the major weaknesses is the lack of suitable metadata,
which strongly limits the possibilty of reusing or repurposing track data and hinders automatization of these processes. 
Furthermore, the lack of provenance information might introduce artefacts in the analysis. 
This lack of proper annotations and of a well-defined and universally adopted metadata standard is
correlated to the lack of a central repository for track data.
This shortfall is mostly due to the fact that track files are often considered additional processed output
connected to the raw data (typically sequence data) which comprises the main part of a published dataset.
The associated track files are, thus, deposited as part of full datasets in larger repositories,
such as [Gene Expression Omnibus (GEO)](https://www.ncbi.nlm.nih.gov/geo/),
[BioStudies](https://www.ebi.ac.uk/biostudies/),
or [The European Phenome-genome Archive (EGA)](https://ega-archive.org/),
and the metadata is organized according to the standards required by these repositories.

The metadata models for annotation of genomic datasets available today evolved
from [INSDSeq](https://www.insdc.org/documents/xml-status),
the official supported XML format of the International Nucleotide Sequence Database Collaboration (INSDC),
a long-standing foundational initiative that operates between NCBI, EMBL-EBI and DDBJ to facilitate genomic data exchange.
INSDC covers the spectrum of data raw reads, though alignments and assemblies to functional annotation, enriched with contextual
information relating to samples and experimental configurations. 
The INSDSeq standard proposes the assignment of a number of interlinked metadata object to each data file,
the most relevant among these being “Experiment”, “Study”, and “Sample”. Modern implementations of this model are the
[ENA metadata model](https://ena-docs.readthedocs.io/en/latest/submit/general-guide/metadata.html) and the
[Sequence Read Archive (SRA) schemas](https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=xml_schemas).

The [International Human Epigenome Consortium (IHEC)](http://ihec-epigenomes.org/) is an umbrella organization bringing together
funding agencies and research teams from around the globe with the aim of coordinating the production of reference maps of human epigenomes.
The data released by IHEC-associated projects, including [BLUEPRINT](http://blueprint-data.bsc.es/) and [ENCODE](https://www.encodeproject.org/),
can be visualized, retrieved and downloaded via the [IHEC data portal](https://epigenomesportal.ca/ihec/index.html). 
The associated metadata is collected according to a model extending the SRA schemas with [additional attribues
and reccomendation for specific ontologies](https://github.com/IHEC/ihec-metadata/blob/master/specs/Ihec_metadata_specification.md).

The [“Functional annotation of animal genomes” (FAANG)](https://www.animalgenome.org/community/FAANG/)
consortium was created to discover basic functional knowledge of genome function to decipher the genotype-to-phenotype (G2P) link in farmed animals.
This community is actively working on standardization of core assays and experimental protocols, coordination and facilitation of data sharing through
its [Data Portal](https://data.faang.org/home), and establishing suitable infrastructures for data analysis. 
The [FAANG metadata specifications](https://github.com/FAANG/dcc-metadata/blob/master/docs/faang_metadata_overview.md).
The metadata is subdivided into 3 related categories: "Samples", "Experiments", and "Analyses".
Metadata is represented atomically and each piece of information described in a separate record with a clear label.
Descriptive factors use ontology terms, with a preference for the [Experimental Factor Ontology (EFO)](https://www.ebi.ac.uk/efo/)
and the ontologies it imports. The FAANG metadata model supports the
[Minimum Information About a Microarray Experiment (MIAME)](https://www.fged.org/projects/miame/) and
[Minimum Information about a high-throughput SEQuencing Experiment (MINSEQE)](https://www.fged.org/projects/minseqe/) guidelines.

The [International Cancer Genome Consortium (ICGC)](https://icgc.org/) coordinates a global network of research groups
that aims to generate and publicly release comprehensive catalogues of genomic, transcriptomic, and epigenomic information
across 50 different cancer types and/or subtypes of clinical and societal importance.
The ICGC also supports the standardization of clinical information reporting and the dissemination of analytical tools to promote
the integration of other datasets with data generated by ICGC member organizations.  
The ICGC Data Coordination Center (DCC) in charge of managing the data for the consortium.
This data adheres to specific formats and restrictions to ensure a standard of quality and correctness,
collected in a document called [Data Dictionary](https://docs.icgc.org/dictionary/viewer/).
Before submission to the ICGC data portal, the user is required to submit the raw sequence data to the
[European Genome-phenome Archive (EGA)](https://ega-archive.org/). This requires the metadata to be
collected according to the
[EGA metadata model](https://docs.icgc.org/submission/guide/overview/submitting-raw-data-ega/#fragment-of-the-sample-xml-file),
comprised of a number of objects including "sample", "experiment", and "run".
ICGC also established the [Pan Cancer Analysis of Whole Genomes (PCAWG)](https://dcc.icgc.org/pcawg),
a federated database where the various partners are responsible for pushing their "patients",
"samples", "experiments" and "analyses" metadata and data, mapped and normalized to the ICGC data models.

The [Genomic Data Commons (GDC)](https://gdc.cancer.gov/about-gdc) is a research program of the National Cancer Institute (NCI),
part of the National Institutes of Health (NIH). The GDC's aim is to provide a data repository
enabling the sharing of data across cancer genomic studies in support of precision medicine.
when submitting sequencing data, the user is required to provide a set of metadata objects compliant with the SRA model.
The GDC hosts and distributes data from The Cancer Genome Atlas (TCGA),
Therapeutically Applicable Research to Generate Effective Treatments (TARGET), and other programs.

##########################

The GDC data model is based off the [DAta Tags Suite (DATS)](https://pubmed.ncbi.nlm.nih.gov/28585923/),
a general metadata model for biological results,
which itself was designed to mirror the [Journal Article Tag Suite (JATS)](https://jats.nlm.nih.gov),
required for submission into [PubMed](https://pubmed.ncbi.nlm.nih.gov/) to index publications.

Finally, the [ISA framework](https://isa-tools.org) provides metadata standards for annotating experimental datasets,
with detailed metadata configurations designed by expert groups for most common experiment types and domains,
represented in ISA-TAB or ISA-JSON formats.
The ISA data model is built around some core metadata categories: Investigation, Study, and Assay.
In addition, the ISA framework provides tools for annotation, curation, or conversion of metadata, and also deployment
tools that follow the requirements of public repositories, such as [ArrayExpress](https://www.ebi.ac.uk/arrayexpress/)
and the [European Nucleotide Archive (ENA)](https://www.ebi.ac.uk/ena/browser/),
as well as selected journals. Noteworthy, the ISA framework has recently been selected as one of the [Recommended
Interoperability Resources](https://elixir-europe.org/platforms/interoperability/rir-selection#:~:text=An%20ELIXIR%20Recommended%20Interoperability%20Resource,thus%20supporting%20the%20FAIR%20Principles.) by ELIXIR.

**here come the FAIRtracks**
Building upon prior art, we started by creating a data model around four key objects:
studies, samples, experiments, and tracks (see Figure 1).
The atomic data element is the Track, a genomic data file,
generally in a binarized and indexed file format, such as BigBedxxv, BigWigxxvi or BCFxxvii,
optimized for display in a genome browser.
Each track is generated by an Experiment, whether physical or *in silico*.
Physical experiments can be mapped for example to experiments in the
[European Genome-phenome Archive (EGA)](https://ega-archive.org/),
although to our knowledge, *in silico* experiments do not have an authoritative identifier system.
Physical experiments link out to Samples, as described in [BioSamples](https://pubmed.ncbi.nlm.nih.gov/30407529/).
Next, a set of experiments is contained within a study, which is akin to the EGA Study objects.
Finally, one or more tracks are grouped into a Track Collection,
which is the outer scope of the model and directly matches the existing track hub object.
A Track Collection can also refer to an ad hoc collection of tracks, e.g., documenting the
input data of published analyses.

We then defined the required and optional attributes for each object type (Table 2).
The first consideration when defining a data model is to strike a compromise between the work
imposed on the producer and the consumer of the metadata. The file must be clear and useful,
yet not be such of a burden to compile that submitters would circumvent metadata entry.
We therefore opted to only require attributes that appeared necessary for generic re-analysis of the data,
while at the same time promoting FAIRness,
by inclusion of resolvable references to relevant existing metadata records in external resources.
To ensure practical usability,
we generated a large test metadata record to test our proposal for ambiguities and difficulties,
as described in more detail below.
This testing phase clarified a need for an automated intermediate step between metadata curation and consumption,
which in most cases simply meant to augment resolvable identifiers with human readable versions of the same.
Adding this augmentation step simplified metadata entry,
while simultaneously strengthened metadata search and extraction, in both automated and manual usage scenarios.

